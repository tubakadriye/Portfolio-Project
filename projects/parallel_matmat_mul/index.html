<!DOCTYPE html>
<html>
    <head>
       <meta charset="utf-8">
       <meta name="viewport" content="width = device-width, initial-scale=1">
       <!-- <link rel="preconnect" href="https://fonts.googleapis.com"> -->
        <!-- <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> -->
        <link href="https://fonts.googleapis.com/css2?family=Abel&family=Medula+One&display=swap" rel="stylesheet"> 
       <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"> -->
       <link rel="stylesheet" href="../bike_sharing/style.css">
       <title>Parallel Matrix Multiplication: - Web Portfolio of Tuba</title> 
    </head>
    <body>
        <nav class="navbar">
            <!-- <div class="container">
                <button class="navbar-toggler" type="button"  data-toggle="collapse" data-target="#navbarMenu">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarMenu"> -->
                    <ul class="navbar-nav">
                        <li><a class="nav-link" href="/Home">Home</a></li>
                        <li><a class="nav-link active" href="/projects">Projects</a></li>
                        <li><a class="nav-link" href="/contact">Contact</a></li>
                    </ul>

                <!-- </div>

            </div> -->

        </nav>
        
        <main>
            <section class="hero-image">
                 <article class="hero-text">
                        <a class="back-to-projects" href="/projects/index.html">Back to Projects</a>
                        <h1 class="title">Parallel Matrix Multiplication:</h1>
                         <p>A parallel algorithm with MPI is written and tested on the matrix sizes 2048, 4096, 6144, 8192,
                           10240, 12288, 14336, 16,384, 18432 and 20480 , for the core numbers 8 , 16, 32, 64 and 128. The
                           matrix sizes are chosen that way to get an even partitioning of matrices across cores. Pointwise and also
                           block-wise matrix multiplication is done with this parallel algorithm and results are compared.</p>
 
                         <p>Here, all matrices are allocated with dynamic memory allocation as one-dimensional arrays to
                           decrease the computation cost. First, memory is allocated in each core for B, A_block, B_block,
                           C_block matrices and in core 0 for matrix C. Matrix A and B are partitioned into cores by generating
                           matrices A_block and B_block inside each core with the size of (SIZE/nb of processors ). Then
                           B_block matrices are gathered in all cores with MPIâ€™s allgatherv() function so each core obtained B
                           matrix. Then inside each core A_block matrices are multiplied by B matrices and stored in C_block
                           matrices. After that, C_block matrices are gathered with the gatherv() function inside core 0. </p>  
                 
                         <p>Time is
                           calculated with MPI_Wtime() function. All the allocated matrices are freed at the end of main().
                           Here, collective communication is used instead of point-to-point communication because it is
                           known that with point-to-point communication, communication cost increases with the increase of
                           the number of cores.</p> 
                         <p>
                           To get faster results, allgatherv and gatherv are used. Since allgather and gather
                           functions gather just the first line of a matrix, vector versions of these are chosen to be used.
                           To do the collective communication with algatherv and gatherv and to get each block matrix
                           B_block within each processor, a new array data type is created with MPI_Type_create_subarray(2,
                           bigsizes, subsizes, starts, MPI_ORDER_C, MPI_DOUBLE, &new_B_block); function. This
                           subarray represents each row of B_block and C_block. After the creation of this new data type, they are resized back to double to be able to do the calculations inside Algatherv and gatherv with
                           MPI_Type_create_resized(new_B_block, 0, sizeof(double), &B_blk);
                         </p>
                            
                         <p id="github"> View this project on Github: 
                            <a href="https://github.com/tubakadriye/Parallel-Matrix-Matrix-Multiplication">https://github.com/tubakadriye/Parallel-Matrix-Matrix-Multiplication</a>  
 
                         </p>  
                                     
                 </article> 
                          
             </section>  
        </main>
        
        <!-- <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script> -->
        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"></script> -->
        <!-- <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"></script> -->
    </body>   
</html>